{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec6b8bb7",
   "metadata": {},
   "source": [
    "## TRL(Transformer Reinforcement Learning) Training with Kubeflow SDK and Advanced Checkpointing\n",
    "\n",
    "This notebook demonstrates how to use the Kubeflow Trainer SDK to create and manage TrainJobs\n",
    "\n",
    "### Features Demonstrated\n",
    "- **Kubeflow SDK Integration**: Programmatic TrainJob creation and management\n",
    "- **Checkpointing**: Controller-managed resume/suspended compatibility for model checkpoints\n",
    "- **TRL SFTTrainer**: Supervised fine-tuning using Peft-LoRA with GPT-2 and Alpaca dataset for instruction following\n",
    "- **Distributed Training**: Multi-node Multi-GPU coordination\n",
    "- **Infra pre-requisite for this demo** : \n",
    "    This demo can be run on -\n",
    "    - CPUs based training using GLOO backend (default configuration)\n",
    "    - GPUs based training using NCCL backend\n",
    "    - Multi-node Multi-GPU distributed training using Trainer V2 MlPolicies (NumNodes/NProcPerNodes)\n",
    "\n",
    "Sample scripts : \n",
    "- mnist.py\n",
    "- trl_training.py\n",
    "\n",
    "### References\n",
    "- [Kubeflow Trainer SDK](https://github.com/kubeflow/sdk)\n",
    "- [TRL Documentation](https://huggingface.co/docs/trl/)\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c67d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Kubeflow SDK from github main branch\n",
    "%pip install git+https://github.com/kubeflow/sdk.git@main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f97773c",
   "metadata": {},
   "source": [
    "### Define TRL Training Function\n",
    "- Progress file writer (callbacks)\n",
    "- Distributed checkpoint coordination\n",
    "- Automated model checkpointing by SIGTERM signal handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932382b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trl_train():    \n",
    "    #!/usr/bin/env python3\n",
    "    \"\"\"\n",
    "    Advanced TRL training script with controller integration and distributed coordination.\n",
    "    Combines controller-managed checkpointing with production-grade distributed training.\n",
    "    \"\"\"\n",
    "    \n",
    "    import os\n",
    "    import json\n",
    "    import time\n",
    "    import signal\n",
    "    import torch\n",
    "    import numpy\n",
    "    from numpy.core.multiarray import _reconstruct\n",
    "    import torch.serialization\n",
    "    import torch.distributed as dist\n",
    "    from datetime import datetime\n",
    "    from pathlib import Path\n",
    "    from typing import Optional\n",
    "    from datasets import load_dataset, load_from_disk\n",
    "    from transformers import (\n",
    "        AutoTokenizer,\n",
    "        TrainingArguments,\n",
    "        TrainerState,\n",
    "        TrainerControl,\n",
    "        TrainerCallback,\n",
    "        set_seed,\n",
    "    )\n",
    "    from transformers.trainer_utils import get_last_checkpoint\n",
    "    from trl import (\n",
    "        ModelConfig,\n",
    "        ScriptArguments,\n",
    "        SFTConfig,\n",
    "        SFTTrainer,\n",
    "        TrlParser,\n",
    "        get_peft_config,\n",
    "    )\n",
    "    \n",
    "    # Safe tensor loading configuration\n",
    "    torch.serialization.add_safe_globals([_reconstruct, numpy.ndarray, numpy.dtype, numpy.dtypes.UInt32DType])\n",
    "    \n",
    "    class ProgressionTracker:\n",
    "        \"\"\"Tracks and writes training progression.\"\"\"\n",
    "\n",
    "        def __init__(\n",
    "            self,\n",
    "            total_epochs: int,\n",
    "            steps_per_epoch: int,\n",
    "            status_file_path: Optional[str] = None,\n",
    "            update_interval: int = 30,\n",
    "        ):\n",
    "            self.total_epochs = total_epochs\n",
    "            self.steps_per_epoch = steps_per_epoch\n",
    "            self.total_steps = total_epochs * steps_per_epoch\n",
    "            self.status_file_path = status_file_path or os.getenv(\n",
    "                \"TRAINJOB_PROGRESSION_FILE_PATH\", \"/tmp/training_progression.json\"\n",
    "            )\n",
    "            self.update_interval = update_interval\n",
    "            self.start_time = time.time()\n",
    "            self.last_update_time = 0\n",
    "            self.current_epoch = 0\n",
    "            self.current_step = 0\n",
    "            self.metrics = {}\n",
    "\n",
    "        def update_step(\n",
    "            self,\n",
    "            epoch: int,\n",
    "            step: int,\n",
    "            loss: float = None,\n",
    "            learning_rate: float = None,\n",
    "            checkpoint_dir: str = None,\n",
    "            **kwargs,\n",
    "        ):\n",
    "            \"\"\"Update current step.\"\"\"\n",
    "            self.current_epoch = epoch\n",
    "            self.current_step = (epoch - 1) * self.steps_per_epoch + step + 1\n",
    "\n",
    "            training_metrics = {}\n",
    "            generic_metrics = {}\n",
    "\n",
    "            if loss is not None:\n",
    "                training_metrics[\"loss\"] = str(loss)\n",
    "            if learning_rate is not None:\n",
    "                training_metrics[\"learning_rate\"] = str(learning_rate)\n",
    "\n",
    "            if checkpoint_dir and os.path.exists(checkpoint_dir):\n",
    "                try:\n",
    "                    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint-') or f.startswith('epoch-')]\n",
    "                    if checkpoints:\n",
    "                        training_metrics[\"checkpoints_stored\"] = len(checkpoints)\n",
    "                        # Find latest checkpoint by highest number\n",
    "                        def get_checkpoint_number(checkpoint_name):\n",
    "                            try:\n",
    "                                if 'checkpoint-' in checkpoint_name:\n",
    "                                    return int(checkpoint_name.split('-')[1].split('.')[0])\n",
    "                                elif 'epoch-' in checkpoint_name:\n",
    "                                    return int(checkpoint_name.split('-')[1].split('.')[0])\n",
    "                                else:\n",
    "                                    return -1\n",
    "                            except (IndexError, ValueError):\n",
    "                                return -1\n",
    "                        \n",
    "                        latest_checkpoint_name = max(checkpoints, key=get_checkpoint_number)\n",
    "                        latest_checkpoint = os.path.join(checkpoint_dir, latest_checkpoint_name)\n",
    "                        training_metrics[\"latest_checkpoint_path\"] = latest_checkpoint\n",
    "                except (OSError, ValueError):\n",
    "                    pass\n",
    "\n",
    "            for key, value in kwargs.items():\n",
    "                str_value = str(value)\n",
    "                \n",
    "                if key in ['accuracy', 'train_accuracy']:\n",
    "                    training_metrics[\"accuracy\"] = str_value\n",
    "                else:\n",
    "                    generic_metrics[key] = str_value\n",
    "\n",
    "            self.training_metrics = training_metrics\n",
    "            self.generic_metrics = generic_metrics\n",
    "\n",
    "            current_time = time.time()\n",
    "            if current_time - self.last_update_time >= self.update_interval:\n",
    "                message = f\"Training step {self.current_step}/{self.total_steps}\"\n",
    "                self.write_status(message)\n",
    "                self.last_update_time = current_time\n",
    "\n",
    "        def update_epoch(self, epoch: int, checkpoint_dir: str = None, **metrics):\n",
    "            \"\"\"Update current epoch.\"\"\"\n",
    "            self.current_epoch = epoch\n",
    "\n",
    "            training_metrics = {}\n",
    "            generic_metrics = {}\n",
    "\n",
    "            for key, value in metrics.items():\n",
    "                str_value = str(value)\n",
    "                \n",
    "                if key in ['loss', 'avg_loss', 'train_loss']:\n",
    "                    training_metrics[\"loss\"] = str_value\n",
    "                elif key in ['accuracy', 'train_accuracy']:\n",
    "                    training_metrics[\"accuracy\"] = str_value\n",
    "                else:\n",
    "                    generic_metrics[key] = str_value\n",
    "\n",
    "            if checkpoint_dir and os.path.exists(checkpoint_dir):\n",
    "                try:\n",
    "                    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint-') or f.startswith('epoch-')]\n",
    "                    if checkpoints:\n",
    "                        training_metrics[\"checkpoints_stored\"] = len(checkpoints)\n",
    "                        # Find latest checkpoint by highest number\n",
    "                        def get_checkpoint_number(checkpoint_name):\n",
    "                            try:\n",
    "                                if 'checkpoint-' in checkpoint_name:\n",
    "                                    return int(checkpoint_name.split('-')[1].split('.')[0])\n",
    "                                elif 'epoch-' in checkpoint_name:\n",
    "                                    return int(checkpoint_name.split('-')[1].split('.')[0])\n",
    "                                else:\n",
    "                                    return -1\n",
    "                            except (IndexError, ValueError):\n",
    "                                return -1\n",
    "                        \n",
    "                        latest_checkpoint_name = max(checkpoints, key=get_checkpoint_number)\n",
    "                        latest_checkpoint = os.path.join(checkpoint_dir, latest_checkpoint_name)\n",
    "                        training_metrics[\"latest_checkpoint_path\"] = latest_checkpoint\n",
    "                except (OSError, ValueError):\n",
    "                    pass\n",
    "\n",
    "            self.training_metrics = training_metrics\n",
    "            self.generic_metrics = generic_metrics\n",
    "\n",
    "            epoch_num = epoch + 1\n",
    "            total_epochs = self.total_epochs\n",
    "            message = f\"Completed epoch {epoch_num}/{total_epochs}\"\n",
    "            self.write_status(message)\n",
    "\n",
    "        def write_status(self, message: str = \"Training in progress\"):\n",
    "            \"\"\"Write training status to file.\"\"\"\n",
    "            try:\n",
    "                current_time = time.time()\n",
    "                \n",
    "                status_data = {\n",
    "                    \"message\": message,\n",
    "                    \"timestamp\": int(current_time),\n",
    "                    \"start_time\": int(self.start_time),\n",
    "                    \"current_step\": self.current_step,\n",
    "                    \"total_steps\": self.total_steps,\n",
    "                    \"current_epoch\": self.current_epoch,\n",
    "                    \"total_epochs\": self.total_epochs,\n",
    "                }\n",
    "                \n",
    "                if self.total_steps > 0:\n",
    "                    percentage = (self.current_step / self.total_steps) * 100\n",
    "                    status_data[\"percentage_complete\"] = f\"{percentage:.2f}\"\n",
    "                    \n",
    "                    if self.current_step > 0:\n",
    "                        elapsed_time = current_time - self.start_time\n",
    "                        time_per_step = elapsed_time / self.current_step\n",
    "                        remaining_steps = self.total_steps - self.current_step\n",
    "                        eta_seconds = int(remaining_steps * time_per_step)\n",
    "                        status_data[\"estimated_time_remaining\"] = eta_seconds\n",
    "                \n",
    "                if hasattr(self, 'training_metrics') and self.training_metrics:\n",
    "                    status_data[\"training_metrics\"] = self.training_metrics\n",
    "                \n",
    "                if hasattr(self, 'generic_metrics') and self.generic_metrics:\n",
    "                    status_data[\"metrics\"] = self.generic_metrics\n",
    "\n",
    "                temp_file = f\"{self.status_file_path}.tmp\"\n",
    "                with open(temp_file, \"w\") as f:\n",
    "                    json.dump(status_data, f, indent=2)\n",
    "                os.rename(temp_file, self.status_file_path)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to write progression status: {e}\")\n",
    "    \n",
    "    # Patch torch.load to handle weights_only parameter and device mapping\n",
    "    original_torch_load = torch.load\n",
    "    def patched_torch_load(*args, **kwargs):\n",
    "        if 'weights_only' not in kwargs:\n",
    "            kwargs['weights_only'] = False\n",
    "        # Add map_location for CPU-only environments or device compatibility\n",
    "        if 'map_location' not in kwargs:\n",
    "            if torch.cuda.is_available():\n",
    "                kwargs['map_location'] = 'cuda'\n",
    "            else:\n",
    "                kwargs['map_location'] = 'cpu'\n",
    "        return original_torch_load(*args, **kwargs)\n",
    "    torch.load = patched_torch_load\n",
    "    \n",
    "    class AdvancedDistributedCheckpointCallback(TrainerCallback):\n",
    "        \"\"\"Distributed SIGTERM handling with progress tracking.\"\"\"\n",
    "        def __init__(self, output_dir: str, progression_tracker: Optional[ProgressionTracker] = None):\n",
    "            self.output_dir = output_dir\n",
    "            self.checkpoint_requested = False\n",
    "            self.save_triggered = False\n",
    "            self.checkpoint_stream = None\n",
    "            self.sigterm_tensor = None\n",
    "            self.progression_tracker = progression_tracker\n",
    "            \n",
    "            self.checkpoint_enabled = os.environ.get('CHECKPOINT_ENABLED', 'false').lower() == 'true'\n",
    "            self.checkpoint_uri = os.environ.get('CHECKPOINT_URI', '/workspace/checkpoints')\n",
    "            \n",
    "            self.progress_file = os.environ.get('TRAINING_PROGRESS_FILE', '/workspace/training_progress.json')\n",
    "            \n",
    "\n",
    "        def _log_message(self, message: str):\n",
    "            \"\"\"Print timestamped message.\"\"\"\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            print(f\"[{timestamp}] {message}\")\n",
    "        \n",
    "        def _write_progress(self, state: TrainerState):\n",
    "            \"\"\"Write progress using ProgressionTracker or fallback.\"\"\"\n",
    "            rank = int(os.environ.get('RANK', '0'))\n",
    "            if rank != 0:\n",
    "                return\n",
    "            \n",
    "            if self.progression_tracker:\n",
    "                try:\n",
    "                    latest_loss = 0.0\n",
    "                    latest_lr = 0.0\n",
    "                    if state.log_history:\n",
    "                        latest_log = state.log_history[-1]\n",
    "                        latest_loss = latest_log.get('loss', latest_log.get('train_loss', latest_log.get('training_loss', 0.0)))\n",
    "                        latest_lr = latest_log.get('learning_rate', latest_log.get('lr', latest_log.get('train_lr', 0.0)))\n",
    "                    \n",
    "                    epoch = int(state.epoch) if state.epoch else 1\n",
    "                    step_in_epoch = state.global_step % self.progression_tracker.steps_per_epoch if self.progression_tracker.steps_per_epoch > 0 else 0\n",
    "                    \n",
    "                    self.progression_tracker.update_step(\n",
    "                        epoch=epoch,\n",
    "                        step=step_in_epoch,\n",
    "                        loss=latest_loss,\n",
    "                        learning_rate=latest_lr,\n",
    "                        checkpoint_dir=self.output_dir,\n",
    "                        global_step=state.global_step,\n",
    "                        max_steps=state.max_steps,\n",
    "                        num_train_epochs=state.num_train_epochs\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"ProgressionTracker update failed: {e}\")\n",
    "                    self._write_simple_progress(state)\n",
    "            else:\n",
    "                self._write_simple_progress(state)\n",
    "        \n",
    "        def _write_simple_progress(self, state: TrainerState):\n",
    "            \"\"\"Fallback progress file writer.\"\"\"\n",
    "            try:\n",
    "                # Extract metrics from trainer state\n",
    "                latest_loss = 0.0\n",
    "                latest_lr = 0.0\n",
    "                if state.log_history:\n",
    "                    latest_log = state.log_history[-1]\n",
    "                    latest_loss = latest_log.get('loss', latest_log.get('train_loss', latest_log.get('training_loss', 0.0)))\n",
    "                    latest_lr = latest_log.get('learning_rate', latest_log.get('lr', latest_log.get('train_lr', 0.0)))\n",
    "                \n",
    "                progress_data = {\n",
    "                    \"epoch\": int(state.epoch) if state.epoch else 1,\n",
    "                    \"totalEpochs\": int(state.num_train_epochs) if state.num_train_epochs else 1,\n",
    "                    \"step\": state.global_step,\n",
    "                    \"totalSteps\": state.max_steps,\n",
    "                    \"loss\": f\"{latest_loss:.4f}\",\n",
    "                    \"learningRate\": f\"{latest_lr:.6f}\",\n",
    "                    \"percentComplete\": f\"{(state.global_step / state.max_steps * 100):.1f}\" if state.max_steps > 0 else \"0.0\",\n",
    "                    \"lastUpdateTime\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
    "                }\n",
    "                \n",
    "                temp_file = self.progress_file + '.tmp'\n",
    "                with open(temp_file, 'w') as f:\n",
    "                    json.dump(progress_data, f, indent=2)\n",
    "                os.rename(temp_file, self.progress_file)\n",
    "                os.chmod(self.progress_file, 0o644)\n",
    "                \n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        def _init_distributed_signal_tensor(self):\n",
    "            \"\"\"Initialize distributed SIGTERM tensor.\"\"\"\n",
    "            try:\n",
    "                if dist.is_initialized():\n",
    "                    device = torch.cuda.current_device() if torch.cuda.is_available() else torch.device('cpu')\n",
    "                    self.sigterm_tensor = torch.zeros(1, dtype=torch.float32, device=device)\n",
    "                    self._log_message(f\"Initialized distributed SIGTERM tensor on device: {device}\")\n",
    "                else:\n",
    "                    self._log_message(\"Distributed training not initialized - using local SIGTERM handling only\")\n",
    "            except Exception as e:\n",
    "                self._log_message(f\"Failed to initialize distributed SIGTERM tensor: {e}. Using local handling only.\")\n",
    "\n",
    "        def _check_distributed_sigterm(self):\n",
    "            \"\"\"Check for distributed SIGTERM.\"\"\"\n",
    "            try:\n",
    "                if dist.is_initialized() and self.sigterm_tensor is not None:\n",
    "                    dist.all_reduce(self.sigterm_tensor, op=dist.ReduceOp.MAX)\n",
    "                    return self.sigterm_tensor.item() > 0.5\n",
    "            except Exception as e:\n",
    "                self._log_message(f\"Distributed SIGTERM check failed: {e}. Using local signal only.\")\n",
    "            return self.checkpoint_requested\n",
    "\n",
    "        def _sigterm_handler(self, signum, frame):\n",
    "            \"\"\"Handle SIGTERM signal.\"\"\"\n",
    "            rank = os.environ.get(\"RANK\", \"-1\")\n",
    "            self._log_message(f\"Rank {rank}: SIGTERM received, flagging for distributed checkpoint.\")\n",
    "            self.checkpoint_requested = True\n",
    "            if self.sigterm_tensor is not None:\n",
    "                self.sigterm_tensor.fill_(1.0)\n",
    "\n",
    "        def on_train_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "            rank = os.environ.get(\"RANK\", \"-1\")\n",
    "            os.makedirs(self.output_dir, exist_ok=True)\n",
    "            self._init_distributed_signal_tensor()\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                self.checkpoint_stream = torch.cuda.Stream()\n",
    "                self._log_message(f\"Rank {rank}: Created dedicated CUDA stream for checkpointing.\")\n",
    "\n",
    "            signal.signal(signal.SIGTERM, self._sigterm_handler)\n",
    "            self._log_message(f\"Rank {rank}: Advanced distributed SIGTERM handler registered.\")\n",
    "\n",
    "            try:\n",
    "                if dist.is_initialized():\n",
    "                    dist.barrier()\n",
    "                    self._log_message(f\"Rank {rank}: Distributed coordination setup synchronized across all ranks\")\n",
    "            except Exception as e:\n",
    "                self._log_message(f\"Rank {rank}: Failed to synchronize distributed setup: {e}\")\n",
    "\n",
    "        def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "            if state.global_step % args.logging_steps == 0:\n",
    "                self._write_progress(state)\n",
    "            \n",
    "            if self.progression_tracker and state.global_step % max(1, args.logging_steps // 2) == 0:\n",
    "                rank = int(os.environ.get('RANK', '0'))\n",
    "                if rank == 0:\n",
    "                    # Extract current metrics\n",
    "                    latest_loss = 0.0\n",
    "                    latest_lr = 0.0\n",
    "                    if state.log_history:\n",
    "                        latest_log = state.log_history[-1]\n",
    "                        latest_loss = latest_log.get('loss', latest_log.get('train_loss', latest_log.get('training_loss', 0.0)))\n",
    "                        latest_lr = latest_log.get('learning_rate', latest_log.get('lr', latest_log.get('train_lr', 0.0)))\n",
    "                    \n",
    "                    epoch = int(state.epoch) if state.epoch else 1\n",
    "                    step_in_epoch = state.global_step % self.progression_tracker.steps_per_epoch if self.progression_tracker.steps_per_epoch > 0 else 0\n",
    "                    \n",
    "                    current_time = time.time()\n",
    "                    elapsed_time = current_time - self.progression_tracker.start_time\n",
    "                    \n",
    "                    batch_size = args.per_device_train_batch_size * args.gradient_accumulation_steps\n",
    "                    if int(os.environ.get('WORLD_SIZE', '1')) > 1:\n",
    "                        batch_size *= int(os.environ.get('WORLD_SIZE', '1'))\n",
    "                    \n",
    "                    total_samples_processed = state.global_step * batch_size\n",
    "                    samples_per_second = total_samples_processed / elapsed_time if elapsed_time > 0 else 0\n",
    "                    \n",
    "                    self.progression_tracker.update_step(\n",
    "                        epoch=epoch,\n",
    "                        step=step_in_epoch,\n",
    "                        loss=latest_loss,\n",
    "                        learning_rate=latest_lr,\n",
    "                        checkpoint_dir=self.output_dir,\n",
    "                        global_step=state.global_step,\n",
    "                        max_steps=state.max_steps,\n",
    "                        train_samples_per_second=f\"{samples_per_second:.2f}\",\n",
    "                        train_runtime=f\"{elapsed_time:.1f}\",\n",
    "                        world_size=os.environ.get('WORLD_SIZE', '1'),\n",
    "                        local_rank=os.environ.get('LOCAL_RANK', '0')\n",
    "                    )\n",
    "                \n",
    "            if self._check_distributed_sigterm() and not self.save_triggered:\n",
    "                rank = os.environ.get(\"RANK\", \"-1\")\n",
    "                self._log_message(f\"Rank {rank}: Distributed SIGTERM detected, initiating checkpoint at step {state.global_step}.\")\n",
    "                self.save_triggered = True\n",
    "                control.should_save = True\n",
    "                control.should_training_stop = True\n",
    "\n",
    "        def on_train_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "            self._write_progress(state)\n",
    "            \n",
    "            if self.progression_tracker:\n",
    "                rank = int(os.environ.get('RANK', '0'))\n",
    "                if rank == 0:\n",
    "                    self.progression_tracker.current_step = self.progression_tracker.total_steps\n",
    "                    self.progression_tracker.write_status(\"Training completed\")\n",
    "            \n",
    "            rank = os.environ.get(\"RANK\", \"-1\")\n",
    "            if rank == \"0\" and self.checkpoint_requested:\n",
    "                self._log_message(f\"Rank {rank}: Training ended due to distributed SIGTERM checkpoint request.\")\n",
    "\n",
    "        def on_epoch_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "            if self.progression_tracker:\n",
    "                rank = int(os.environ.get('RANK', '0'))\n",
    "                if rank == 0:\n",
    "                    epoch = int(state.epoch) if state.epoch else 1\n",
    "                    latest_loss = 0.0\n",
    "                    if state.log_history:\n",
    "                        latest_log = state.log_history[-1]\n",
    "                        latest_loss = latest_log.get('loss', latest_log.get('train_loss', latest_log.get('training_loss', 0.0)))\n",
    "                    \n",
    "                    self.progression_tracker.update_epoch(\n",
    "                        epoch=epoch,\n",
    "                        checkpoint_dir=self.output_dir,\n",
    "                        avg_loss=latest_loss,\n",
    "                        global_step=state.global_step,\n",
    "                        max_steps=state.max_steps\n",
    "                    )\n",
    "\n",
    "        def on_save(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "            rank = os.environ.get(\"RANK\", \"-1\")\n",
    "            if rank == \"0\":\n",
    "                if self.progression_tracker:\n",
    "                    try:\n",
    "                        trainer_state_path = os.path.join(args.output_dir, f\"checkpoint-{state.global_step}\", 'trainer_state.json')\n",
    "                        if os.path.exists(trainer_state_path):\n",
    "                            with open(trainer_state_path, 'r') as f:\n",
    "                                trainer_state_data = json.load(f)\n",
    "                            \n",
    "                            trainer_state_data['training_start_time'] = self.progression_tracker.start_time\n",
    "                            \n",
    "                            with open(trainer_state_path, 'w') as f:\n",
    "                                json.dump(trainer_state_data, f, indent=2)\n",
    "                            \n",
    "                            self._log_message(f\"Rank {rank}: Saved training start time to checkpoint.\")\n",
    "                    except Exception as e:\n",
    "                        self._log_message(f\"Rank {rank}: Failed to save training start time: {e}\")\n",
    "                \n",
    "                self._log_message(f\"Rank {rank}: Checkpoint save completed.\")\n",
    "                if self.checkpoint_requested:\n",
    "                    self._log_message(f\"Rank {rank}: Distributed SIGTERM-triggered checkpoint save finished successfully.\")\n",
    "    \n",
    "    def setup_distributed():\n",
    "        \"\"\"Initialize distributed training.\"\"\"\n",
    "        node_rank = int(os.getenv('PET_NODE_RANK', '0'))\n",
    "        num_nodes = int(os.getenv('PET_NNODES', '1'))\n",
    "        nproc_per_node = int(os.getenv('PET_NPROC_PER_NODE', '1'))\n",
    "        master_addr = os.getenv('PET_MASTER_ADDR', 'localhost')\n",
    "        master_port = os.getenv('PET_MASTER_PORT', '29500')\n",
    "        \n",
    "        local_rank = int(os.getenv('LOCAL_RANK', '0'))\n",
    "        world_size = num_nodes * nproc_per_node\n",
    "        global_rank = node_rank * nproc_per_node + local_rank\n",
    "        \n",
    "        os.environ['RANK'] = str(global_rank)\n",
    "        os.environ['WORLD_SIZE'] = str(world_size)\n",
    "        os.environ['LOCAL_RANK'] = str(local_rank)\n",
    "        os.environ['MASTER_ADDR'] = master_addr\n",
    "        os.environ['MASTER_PORT'] = master_port\n",
    "        \n",
    "        if world_size > 1:\n",
    "            try:\n",
    "                torch.distributed.init_process_group(\n",
    "                    backend='gloo',\n",
    "                    rank=global_rank,\n",
    "                    world_size=world_size\n",
    "                )\n",
    "                torch.distributed.barrier()\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to initialize distributed training: {e}\")\n",
    "        \n",
    "        return local_rank, global_rank, world_size\n",
    "    \n",
    "    def load_dataset_from_initializer():\n",
    "        \"\"\"Load dataset from initializer or download.\"\"\"\n",
    "        dataset_dir = Path(\"/workspace/dataset\")\n",
    "        \n",
    "        if dataset_dir.exists() and any(dataset_dir.iterdir()):\n",
    "            try:\n",
    "                full_dataset = load_from_disk(str(dataset_dir))\n",
    "                if isinstance(full_dataset, dict):\n",
    "                    train_dataset = full_dataset.get('train', full_dataset.get('train[:100]'))\n",
    "                    test_dataset = full_dataset.get('test', full_dataset.get('test[:20]'))\n",
    "                else:\n",
    "                    train_size = min(100, len(full_dataset) - 20)\n",
    "                    train_dataset = full_dataset.select(range(train_size))\n",
    "                    test_dataset = full_dataset.select(range(train_size, min(train_size + 20, len(full_dataset))))\n",
    "                \n",
    "                return train_dataset, test_dataset\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load from initializer: {e}\")\n",
    "        \n",
    "        dataset_name = os.getenv('DATASET_NAME', 'tatsu-lab/alpaca')\n",
    "        train_split = os.getenv('DATASET_TRAIN_SPLIT', 'train[:100]')\n",
    "        test_split = os.getenv('DATASET_TEST_SPLIT', 'train[100:120]')\n",
    "        \n",
    "        train_dataset = load_dataset(dataset_name, split=train_split)\n",
    "        test_dataset = load_dataset(dataset_name, split=test_split)\n",
    "        \n",
    "        return train_dataset, test_dataset\n",
    "    \n",
    "    def load_model_from_initializer():\n",
    "        \"\"\"Load model and tokenizer.\"\"\"\n",
    "        model_dir = Path(\"/workspace/model\")\n",
    "        \n",
    "        if model_dir.exists() and any(model_dir.iterdir()):\n",
    "            model_path = str(model_dir)\n",
    "        else:\n",
    "            model_path = os.getenv('MODEL_NAME', 'gpt2')\n",
    "        \n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=True)\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            if tokenizer.chat_template is None:\n",
    "                tokenizer.chat_template = (\n",
    "                    \"{% for message in messages %}\"\n",
    "                    \"{% if message['role'] == 'user' %}\"\n",
    "                    \"### Instruction:\\n{{ message['content'] }}\\n\"\n",
    "                    \"{% elif message['role'] == 'assistant' %}\"\n",
    "                    \"### Response:\\n{{ message['content'] }}{{ eos_token }}\\n\"\n",
    "                    \"{% endif %}\"\n",
    "                    \"{% endfor %}\"\n",
    "                )\n",
    "            \n",
    "            return model_path, tokenizer\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            model_path = 'gpt2'\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            return model_path, tokenizer\n",
    "    \n",
    "    def prepare_datasets(train_dataset, test_dataset, tokenizer):\n",
    "        \"\"\"Prepare datasets for training.\"\"\"\n",
    "        def template_dataset(sample):\n",
    "            if 'instruction' in sample and 'output' in sample:\n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": sample['instruction']},\n",
    "                    {\"role\": \"assistant\", \"content\": sample['output']},\n",
    "                ]\n",
    "            elif 'question' in sample and 'answer' in sample:\n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": sample['question']},\n",
    "                    {\"role\": \"assistant\", \"content\": sample['answer']},\n",
    "                ]\n",
    "            else:\n",
    "                content = str(sample.get('text', sample.get('content', 'Sample text')))\n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": \"Complete this text:\"},\n",
    "                    {\"role\": \"assistant\", \"content\": content},\n",
    "                ]\n",
    "            \n",
    "            return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False)}\n",
    "        \n",
    "        train_columns = list(train_dataset.features.keys())\n",
    "        train_columns.remove('text') if 'text' in train_columns else None\n",
    "        \n",
    "        train_dataset = train_dataset.map(template_dataset, remove_columns=train_columns)\n",
    "        \n",
    "        if test_dataset is not None:\n",
    "            test_columns = list(test_dataset.features.keys())\n",
    "            test_columns.remove('text') if 'text' in test_columns else None\n",
    "            test_dataset = test_dataset.map(template_dataset, remove_columns=test_columns)\n",
    "        \n",
    "        return train_dataset, test_dataset\n",
    "    \n",
    "    def get_training_parameters():\n",
    "        \"\"\"Get training parameters.\"\"\"\n",
    "        checkpoint_dir = Path(os.getenv('CHECKPOINT_URI', '/workspace/checkpoints'))\n",
    "        checkpoint_enabled = os.getenv('CHECKPOINT_ENABLED', 'false').lower() == 'true'\n",
    "        checkpoint_interval = os.getenv('CHECKPOINT_INTERVAL', '30s')\n",
    "        max_checkpoints = int(os.getenv('CHECKPOINT_MAX_RETAIN', '5'))\n",
    "        \n",
    "        parameters = {\n",
    "            'model_name_or_path': os.getenv('MODEL_NAME', 'gpt2'),\n",
    "            'model_revision': 'main',\n",
    "            'torch_dtype': 'bfloat16',\n",
    "            'use_peft': True,\n",
    "            'lora_r': int(os.getenv('LORA_R', '16')),\n",
    "            'lora_alpha': int(os.getenv('LORA_ALPHA', '32')),\n",
    "            'lora_dropout': float(os.getenv('LORA_DROPOUT', '0.1')),\n",
    "            'lora_target_modules': ['c_attn', 'c_proj'],  # GPT-2 specific\n",
    "            'dataset_name': os.getenv('DATASET_NAME', 'tatsu-lab/alpaca'),\n",
    "            'dataset_config': 'main',\n",
    "            'dataset_train_split': os.getenv('DATASET_TRAIN_SPLIT', 'train[:100]'),\n",
    "            'dataset_test_split': os.getenv('DATASET_TEST_SPLIT', 'train[100:120]'),\n",
    "            'max_seq_length': int(os.getenv('MAX_SEQ_LENGTH', '512')),\n",
    "            'num_train_epochs': int(os.getenv('MAX_EPOCHS', '3')),\n",
    "            'per_device_train_batch_size': int(os.getenv('BATCH_SIZE', '2')),\n",
    "            'per_device_eval_batch_size': int(os.getenv('BATCH_SIZE', '2')),\n",
    "            'eval_strategy': 'steps',\n",
    "            'eval_steps': int(os.getenv('EVAL_STEPS', '25')),\n",
    "            'bf16': torch.cuda.is_available(),  # Only use bf16 if CUDA is available\n",
    "            'fp16': not torch.cuda.is_available(),  # Use fp16 for CPU training\n",
    "            'learning_rate': float(os.getenv('LEARNING_RATE', '5e-5')),\n",
    "            'warmup_steps': int(os.getenv('WARMUP_STEPS', '10')),\n",
    "            'lr_scheduler_type': 'cosine',\n",
    "            'optim': 'adamw_torch',\n",
    "            'max_grad_norm': 1.0,\n",
    "            'seed': 42,\n",
    "            'gradient_accumulation_steps': int(os.getenv('GRADIENT_ACCUMULATION_STEPS', '4')),\n",
    "            'save_strategy': 'steps',\n",
    "            'save_steps': int(os.getenv('SAVE_STEPS', '20')),\n",
    "            'save_total_limit': max_checkpoints if checkpoint_enabled else None,\n",
    "            'logging_strategy': 'steps',\n",
    "            'logging_steps': int(os.getenv('LOGGING_STEPS', '5')),\n",
    "            'report_to': [],\n",
    "            'output_dir': str(checkpoint_dir),\n",
    "        }\n",
    "        \n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "\n",
    "    \"\"\"Main training function.\"\"\"\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    local_rank, global_rank, world_size = setup_distributed()\n",
    "    \n",
    "    if world_size > 1:\n",
    "        try:\n",
    "            if dist.is_initialized():\n",
    "                dist.barrier()\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to synchronize distributed setup: {e}\")\n",
    "    \n",
    "    os.makedirs(\"/workspace/cache/transformers\", exist_ok=True)\n",
    "    os.makedirs(\"/workspace/cache\", exist_ok=True)\n",
    "    os.makedirs(\"/workspace/cache/datasets\", exist_ok=True)\n",
    "    \n",
    "    parameters = get_training_parameters()\n",
    "    checkpoint_dir = Path(parameters['output_dir'])\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    parser = TrlParser((ScriptArguments, SFTConfig, ModelConfig))\n",
    "    script_args, training_args, model_args = parser.parse_dict(parameters)\n",
    "    \n",
    "    set_seed(training_args.seed)\n",
    "    \n",
    "    model_path, tokenizer = load_model_from_initializer()\n",
    "    train_dataset, test_dataset = load_dataset_from_initializer()\n",
    "    train_dataset, test_dataset = prepare_datasets(train_dataset, test_dataset, tokenizer)\n",
    "    \n",
    "    progression_tracker = None\n",
    "    \n",
    "    callbacks = [\n",
    "        AdvancedDistributedCheckpointCallback(str(checkpoint_dir), progression_tracker)  # Advanced distributed coordination with progress tracking\n",
    "    ]\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model_path,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        peft_config=get_peft_config(model_args),\n",
    "        processing_class=tokenizer,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    \n",
    "    if trainer.accelerator.is_main_process and hasattr(trainer.model, \"print_trainable_parameters\"):\n",
    "        trainer.model.print_trainable_parameters()\n",
    "    \n",
    "    checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    resume_from_epoch = 0\n",
    "    resume_from_step = 0\n",
    "    \n",
    "    if checkpoint is not None:\n",
    "        try:\n",
    "            checkpoint_files = os.listdir(checkpoint)\n",
    "            if 'trainer_state.json' not in checkpoint_files:\n",
    "                checkpoint = None\n",
    "            else:\n",
    "                trainer_state_path = os.path.join(checkpoint, 'trainer_state.json')\n",
    "                if os.path.exists(trainer_state_path):\n",
    "                    with open(trainer_state_path, 'r') as f:\n",
    "                        trainer_state = json.load(f)\n",
    "                        resume_from_epoch = int(trainer_state.get('epoch', 0))\n",
    "                        resume_from_step = int(trainer_state.get('global_step', 0))\n",
    "                        print(f\"Resuming from checkpoint: epoch {resume_from_epoch}, step {resume_from_step}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Checkpoint validation failed: {e}\")\n",
    "            checkpoint = None\n",
    "    \n",
    "    if world_size == 1 or global_rank == 0:\n",
    "        train_dataset_size = len(train_dataset) if train_dataset else 1000  # fallback\n",
    "        batch_size = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps\n",
    "        if world_size > 1:\n",
    "            batch_size *= world_size\n",
    "        steps_per_epoch = max(1, train_dataset_size // batch_size)\n",
    "        \n",
    "        num_epochs = int(training_args.num_train_epochs)\n",
    "        start_epoch_for_calculation = max(1, resume_from_epoch) if resume_from_epoch > 0 else 1\n",
    "        total_epochs_planned = start_epoch_for_calculation + num_epochs - 1\n",
    "        \n",
    "        progression_tracker = ProgressionTracker(\n",
    "            total_epochs=total_epochs_planned,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            update_interval=int(os.getenv('PROGRESSION_UPDATE_INTERVAL', '10'))\n",
    "        )\n",
    "        \n",
    "        if checkpoint is not None and (resume_from_epoch > 0 or resume_from_step > 0):\n",
    "            completed_epochs = resume_from_epoch if resume_from_epoch > 0 else 0\n",
    "            progression_tracker.current_epoch = completed_epochs\n",
    "            progression_tracker.current_step = completed_epochs * steps_per_epoch\n",
    "            \n",
    "            try:\n",
    "                trainer_state_path = os.path.join(checkpoint, 'trainer_state.json')\n",
    "                if os.path.exists(trainer_state_path):\n",
    "                    with open(trainer_state_path, 'r') as f:\n",
    "                        trainer_state = json.load(f)\n",
    "                        if 'training_start_time' in trainer_state:\n",
    "                            progression_tracker.start_time = trainer_state['training_start_time']\n",
    "                            print(f\"Restored original training start time from checkpoint\")\n",
    "                        else:\n",
    "                            current_time = time.time()\n",
    "                            if progression_tracker.current_step > 0 and progression_tracker.total_steps > 0:\n",
    "                                progress_ratio = progression_tracker.current_step / progression_tracker.total_steps\n",
    "                                estimated_elapsed = progress_ratio * (progression_tracker.total_steps * 30)\n",
    "                                progression_tracker.start_time = current_time - estimated_elapsed\n",
    "                                print(f\"Estimated original training start time based on progress\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not restore training start time: {e}\")\n",
    "            \n",
    "            progression_tracker.write_status(f\"Training resumed from epoch {resume_from_epoch}\")\n",
    "        else:\n",
    "            progression_tracker.current_epoch = 0\n",
    "            progression_tracker.current_step = 0\n",
    "            progression_tracker.write_status(\"Training started\")\n",
    "    \n",
    "    if callbacks and len(callbacks) > 0:\n",
    "        callbacks[0].progression_tracker = progression_tracker\n",
    "    \n",
    "    if world_size > 1:\n",
    "        try:\n",
    "            if dist.is_initialized():\n",
    "                dist.barrier()\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to synchronize distributed processes: {e}\")\n",
    "    \n",
    "    try:\n",
    "        trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {e}\")\n",
    "        if checkpoint is not None:\n",
    "            try:\n",
    "                if progression_tracker:\n",
    "                    progression_tracker.current_epoch = 0\n",
    "                    progression_tracker.current_step = 0\n",
    "                    progression_tracker.write_status(\"Training restarted from scratch after checkpoint failure\")\n",
    "                trainer.train(resume_from_checkpoint=None)\n",
    "            except Exception as retry_e:\n",
    "                print(f\"Training failed even from scratch: {retry_e}\")\n",
    "                raise retry_e\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    trainer.save_model(training_args.output_dir)\n",
    "    \n",
    "    if progression_tracker and (world_size == 1 or global_rank == 0):\n",
    "        progression_tracker.current_step = progression_tracker.total_steps\n",
    "        progression_tracker.write_status(\"Training completed successfully\")\n",
    "        \n",
    "        print(\"Waiting for progression status to be captured...\")\n",
    "        time.sleep(30)\n",
    "    \n",
    "    if world_size > 1:\n",
    "        try:\n",
    "            if dist.is_initialized():\n",
    "                dist.destroy_process_group()\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to cleanup distributed process group: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c002e5e",
   "metadata": {},
   "source": [
    "### Create TrainJob Using Kubeflow SDK\n",
    "Now we'll use the Kubeflow SDK to create a TrainJob\n",
    "- Training arguments\n",
    "- *CustomTrainer* with the TRL training function\n",
    "- *Initializer* for dataset and model (V2 initializers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b94b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.trainer import CustomTrainer, Initializer\n",
    "\n",
    "training_args = {\n",
    "    # Training hyperparameters (matching trl-trainjob.yaml)\n",
    "    \"LEARNING_RATE\": \"5e-5\",\n",
    "    \"BATCH_SIZE\": \"1\",\n",
    "    \"MAX_EPOCHS\": \"10\",\n",
    "    \"WARMUP_STEPS\": \"5\",\n",
    "    \"EVAL_STEPS\": \"10\",\n",
    "    \"SAVE_STEPS\": \"5\",\n",
    "    \"LOGGING_STEPS\": \"2\",\n",
    "    \"GRADIENT_ACCUMULATION_STEPS\": \"2\",\n",
    "    \n",
    "    # Model configuration\n",
    "    \"MODEL_NAME\": \"gpt2\",\n",
    "    \"LORA_R\": \"16\",\n",
    "    \"LORA_ALPHA\": \"32\",\n",
    "    \"LORA_DROPOUT\": \"0.1\",\n",
    "    \"MAX_SEQ_LENGTH\": \"512\",\n",
    "    \n",
    "    # Dataset configuration\n",
    "    \"DATASET_NAME\": \"tatsu-lab/alpaca\",\n",
    "    \"DATASET_TRAIN_SPLIT\": \"train[:500]\",\n",
    "    \"DATASET_TEST_SPLIT\": \"train[500:520]\",\n",
    "    \n",
    "    # Checkpointing configuration\n",
    "    \"CHECKPOINT_URI\": \"/workspace/checkpoints\",\n",
    "    \"TRAINJOB_PROGRESSION_FILE_PATH\": \"/workspace/checkpoints/training_progression.json\",\n",
    "    \n",
    "    # Cache directories\n",
    "    \"PYTHONUNBUFFERED\": \"1\",\n",
    "    \"TRANSFORMERS_CACHE\": \"/workspace/cache/transformers\",\n",
    "    \"HF_HOME\": \"/workspace/cache\",\n",
    "    \"HF_DATASETS_CACHE\": \"/workspace/cache/datasets\",\n",
    "    \n",
    "    # Distributed training debug\n",
    "    \"NCCL_DEBUG\": \"INFO\",\n",
    "    \"NCCL_DEBUG_SUBSYS\": \"ALL\",\n",
    "    \"NCCL_SOCKET_IFNAME\": \"eth0\",\n",
    "    \"NCCL_IB_DISABLE\": \"1\",\n",
    "    \"NCCL_P2P_DISABLE\": \"1\",\n",
    "    \"NCCL_TREE_THRESHOLD\": \"0\",\n",
    "    \"TORCH_DISTRIBUTED_DEBUG\": \"INFO\",\n",
    "    \"TORCH_SHOW_CPP_STACKTRACES\": \"1\",\n",
    "}\n",
    "\n",
    "# Create CustomTrainer configuration\n",
    "custom_trainer = CustomTrainer(\n",
    "    func=trl_train,\n",
    "    func_args=training_args,\n",
    "    num_nodes=2,  # Distributed training across 2 nodes\n",
    "    resources_per_node={\n",
    "        \"cpu\": \"2\",\n",
    "        \"memory\": \"4Gi\",\n",
    "        # Uncomment for GPU training:\n",
    "        # \"nvidia.com/gpu\": \"1\",\n",
    "    },\n",
    "    packages_to_install=[\n",
    "        \"transformers[torch]\",\n",
    "        \"trl\", \n",
    "        \"peft\", \n",
    "        \"datasets\", \n",
    "        \"accelerate\",\n",
    "        \"torch\",\n",
    "        \"numpy\"\n",
    "    ],\n",
    "    env={\n",
    "        \"PYTHONUNBUFFERED\": \"1\",\n",
    "        \"NCCL_DEBUG\": \"INFO\",\n",
    "        \"TORCH_DISTRIBUTED_DEBUG\": \"INFO\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Configure Initializers\n",
    "initializer = Initializer(\n",
    "    dataset=Initializer.HuggingFaceDatasetInitializer(\n",
    "        storage_uri=\"hf://tatsu-lab/alpaca\"\n",
    "    ),\n",
    "    model=Initializer.HuggingFaceModelInitializer(\n",
    "        storage_uri=\"hf://gpt2\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e632ece",
   "metadata": {},
   "source": [
    "### Initialize Trainer Client\n",
    "Use token authentication to intialize a training client and list available runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31077051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.trainer import TrainerClient\n",
    "from kubernetes import client\n",
    "\n",
    "api_server = \"\"\n",
    "token = \"\"\n",
    "\n",
    "configuration = client.Configuration()\n",
    "configuration.host = api_server\n",
    "configuration.api_key = {\"authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "# Un-comment if your cluster API server uses a self-signed certificate or an un-trusted CA\n",
    "configuration.verify_ssl = False\n",
    "\n",
    "api_client = client.ApiClient(configuration)\n",
    "trainer_client = TrainerClient(namespace=\"<test-namespace>\", client_configuration=api_client.configuration)\n",
    "\n",
    "print(\"Available runtimes :\", len(client.list_runtimes()))\n",
    "for r in client.list_runtimes():\n",
    "    print(f\"Runtime:r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373b1160",
   "metadata": {},
   "source": [
    "### Create TrainJob\n",
    "Create a TrainJob using resources declared above - \n",
    "- Custom trainer\n",
    "- Dataset & Model initailisers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab961c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = trainer_client.train(\n",
    "    trainer=custom_trainer,\n",
    "    initializer=initializer,\n",
    "    labels={\n",
    "        \"app.kubernetes.io/name\": \"trl-demo\",\n",
    "        \"app.kubernetes.io/component\": \"training\",\n",
    "        \"experiment\": \"advanced-controller-checkpointing\"\n",
    "    },\n",
    "    annotations={\n",
    "        \"training.kubeflow.org/description\": \"TRL GPT-2 fine-tuning with advanced checkpointing using Kubeflow SDK\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805900d1",
   "metadata": {},
   "source": [
    "### Start monitoring - View Training Logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e2a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training logs\n",
    "try:    \n",
    "    # Get logs from the training nodes\n",
    "    logs = trainer_client.get_job_logs(job_name, follow=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING LOGS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Display logs from all nodes\n",
    "    for node_name, node_logs in logs.items():\n",
    "        print(f\"\\n--- {node_name.upper()} LOGS ---\")\n",
    "        # Display last 50 lines of logs\n",
    "        log_lines = node_logs.split('\\n')\n",
    "        for line in log_lines[-50:]:\n",
    "            if line.strip():\n",
    "                print(line)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error getting logs: {e}\")\n",
    "    print(\"Note: Logs may not be available yet if training is still starting up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dae52b",
   "metadata": {},
   "source": [
    "### Cleanup Using Kubeflow SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a2b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the TrainJob when done\n",
    "def cleanup_trainjob():\n",
    "    \"\"\"Clean up the TrainJob using Kubeflow SDK\"\"\"\n",
    "    try:\n",
    "        trainer_client.delete_job(job_name)\n",
    "        print(f\"TrainJob '{job_name}' deleted successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting TrainJob: {e}\")\n",
    "\n",
    "# Get final job status before cleanup\n",
    "try:\n",
    "    final_job = trainer_client.get_job(job_name)\n",
    "    print(f\"Final TrainJob Status:\")\n",
    "    print(f\"   Name: {final_job.name}\")\n",
    "    print(f\"   Status: {final_job.status}\")\n",
    "    print(f\"   Created: {final_job.creation_timestamp}\")\n",
    "    print(f\"   Nodes: {final_job.num_nodes}\")\n",
    "    print(f\"   Runtime: {final_job.runtime.name}\")\n",
    "    \n",
    "    if final_job.steps:\n",
    "        print(f\"   Steps:\")\n",
    "        for step in final_job.steps:\n",
    "            print(f\"     - {step.name}: {step.status}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error getting final job status: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
